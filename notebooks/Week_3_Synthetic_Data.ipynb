{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajilsaj/FICOchallenge/blob/main/notebooks/Week_3_Synthetic_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR6rSC69Y5Eo"
      },
      "source": [
        "# **FICO Educational Analytics Challenge ¬© Fair Isaac 2025**\n",
        "\n",
        "Copyright 2025 FICO licensed under CC BY-NC-SA 4.0. To view a copy of this license, visit https://creativecommons.org/licenses/by-nc-sa/4.0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V-PYMA7Y5An"
      },
      "source": [
        "# Week 3: Synthetic Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QuteW4IY47A"
      },
      "source": [
        "This notebook walks through **best practices and practical code** for generating high-quality synthetic data to fine-tune a banking chatbot.\n",
        "\n",
        "**Why synthetic data?**\n",
        "\n",
        "- Privacy: avoid storing or exposing real customer data.\n",
        "\n",
        "- Coverage: fill rare/edge cases and long-tail requests.\n",
        "\n",
        "- Control: balance class distributions and ensure label quality.\n",
        "\n",
        "\n",
        "**What we'll cover:**\n",
        "\n",
        "1. Model(ing) Choices\n",
        "\n",
        "2. Intents schema\n",
        "\n",
        "3. Data augmentation (Temperature, Sentiments, Speech Style)\n",
        "\n",
        "4. Negative/OOS & ambiguous examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_TRWXTaFGSa"
      },
      "source": [
        "### Expected File Structure\n",
        "\n",
        "This notebook expects you to have the following file structure inside of **MyDrive**:\n",
        "\n",
        "```\n",
        "MyDrive\n",
        "    ‚îî‚îÄ‚îÄ FICO Analytic Challenge\n",
        "        ‚îî‚îÄ‚îÄ Data\n",
        "            ‚îî‚îÄ‚îÄ collections_intents.csv\n",
        "            ‚îî‚îÄ‚îÄ seed_scenarios.csv\n",
        "            ‚îî‚îÄ‚îÄ LLM_Data_Generation.ipynb\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K6qUNflZBO3"
      },
      "source": [
        "## What is \"unsloth\"?\n",
        "\n",
        "Unsloth is an open-source library designed to make fine-tuning large language models (LLMs) faster and more memory-efficient, often by leveraging optimizations like parameter-efficient tuning. From their website:\n",
        "\n",
        "> By manually deriving all compute heavy maths steps and handwriting GPU kernels, Unsloth magically makes training faster without any hardware changes.\n",
        "> [Models run] 10x faster on a single GPU and up to 30x faster on multiple GPU systems compared to Flash Attention 2 (FA2). We support NVIDIA GPUs from Tesla T4 to H100, and we‚Äôre portable to AMD and Intel GPUs.\n",
        "\n",
        "This is especially convenient for google colab, as we have free access to their T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMZhh7Wkr5eg",
        "outputId": "59667260-684d-427b-a72d-2d9b20308c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.12/dist-packages (2026.2.1)\n",
            "Requirement already satisfied: unsloth_zoo>=2026.2.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2026.2.1)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.46.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (26.0)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.10.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.25.0+cu128)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.0.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.6)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.0.35)\n",
            "Requirement already satisfied: bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.49.2)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.6.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
            "Requirement already satisfied: datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.3.0)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.12.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.18.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.6)\n",
            "Requirement already satisfied: trl!=0.19.0,<=0.24.0,>=0.18.2 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.24.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.24.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (23.0.1)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.1.3)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.4.0->unsloth) (1.3.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3->unsloth) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3->unsloth) (0.22.2)\n",
            "Requirement already satisfied: torchao>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.2.1->unsloth) (0.16.0)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.2.1->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.2.1->unsloth) (11.3.0)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.2.1->unsloth) (0.20.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGEXJ-eYmGDp",
        "outputId": "9cd25851-9d21-4032-ad6b-b18af7b73daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import nbformat\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# Base path for your project\n",
        "path = '/content/drive/MyDrive/FICO Analytic Challenge/'\n",
        "\n",
        "# Folder that's holding dataset\n",
        "data = 'Data'\n",
        "\n",
        "# Path to the \"Data\" and \"Model\" folder\n",
        "data_path = os.path.join(path, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI-sJTcHnTUm",
        "outputId": "c02976d1-66ea-4c41-9a4e-4ff1a857f664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G_ThZeU_1HF"
      },
      "outputs": [],
      "source": [
        "# Pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWxjH75L_CoX",
        "outputId": "6458667c-7884-40fe-97e7-234050748158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4 is available.\n"
          ]
        }
      ],
      "source": [
        "# Checking GPU compatibility\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
        "else:\n",
        "    print(\"No GPU available. Training will run on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLmGzzBh_6PF",
        "outputId": "cce0423b-c0ee-4412-8021-66b1ee2923a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE_TYPE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE_TYPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ksAp9ZlZJrl"
      },
      "source": [
        "## While there are many models to choose from for this task, `Llama-3.1-8B-Instruct` is ideal for many reasons:\n",
        "\n",
        "* Instruction-Tuned for Following Prompts\n",
        "\n",
        "  - The Instruct variant is optimized to follow natural-language instructions, making it well-suited for tasks like ‚ÄúGenerate 10 variations of a user asking to check their account balance.‚Äù This reduces the need for convoluted prompt engineering.\n",
        "\n",
        "* Balanced Size vs. Capability\n",
        "\n",
        "  - At 8B parameters, it is large enough to capture rich linguistic variation and domain knowledge, but still lightweight compared to 70B+ models. This balance makes it practical for synthetic data generation while being deployable on our modest hardware.\n",
        "\n",
        "* High-Quality Language Generation\n",
        "\n",
        "  - Produces diverse, coherent, and natural-sounding outputs. This helps simulate realistic user queries, including informal phrasing and coloquial style.\n",
        "\n",
        "* Open and Customizable\n",
        "\n",
        "  - Being open-source, it can be fine-tuned or adapted to banking-specific terminology and intents if needed. This flexibility allows tailoring the synthetic data pipeline directly to our chatbot‚Äôs domain!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6bc82bad87234a09b53e1315e053e70a",
            "a70612f74a75497fbbcc4b5769f5c017",
            "62f37891ed694a0b8fff1af101b44028",
            "0cf692ad1b544571a41dd4bf26eaa7d5",
            "75e09ddc4d074c79a5f39dc823b481d5",
            "0a5d1d0dcd224753b8991f0a181ced55",
            "8631f7c3e53a4e87a0aac72e2e7625cb",
            "bdcd8783b37745efbaed55661f067002",
            "74419781296c4d5a9a914a4ed8c253ac",
            "8b0928dadd2b4f55a52459c153ea53a3",
            "5a2df4436b1446b7b1899080304f2400",
            "713e64c383ed43629aec89d8ddddd171",
            "9692d05e03374ebb9fae4ece2f9d1239",
            "0b6213074ff34e2ca290bcbb54097571",
            "258efc55eb2341d3be2a532b4952dda0",
            "15522af06a144d83987c8744cafefded",
            "f3c9c6baa346445e9c6c4023d8bb4133",
            "3146fd1875114a0ab65b3d86a7aae933",
            "d64a2a677636453ea7fd4d90d1410c1b",
            "d28b1ce5feb4461e8422a58cde1a27eb",
            "627f4acb619d4dbda1e8944dfb454058",
            "fa27ef847bb443fab6398dc2db031689",
            "038e51b470644a249393e569068a5899",
            "2cc292b3afa641128c4c7b83e87784ab",
            "762fce13be6c4a849b1735c6e95dda5b",
            "293965b12884494ca3eb61a450df3240",
            "c3b0aeaa825348b2a274ce478da10774",
            "1a103466df2249a1bd5e13843b953f46",
            "124b63a5610b44e3a889e4e2e45c65fc",
            "edad38bf2bd347b6a38b7b6bb9826d53",
            "cfdc5daca3614aa6b687b9554b6f2a04",
            "c71cf356fb6a48239a7dfb46522673b9",
            "e0a7196b99b340d4a45e2ed0500497cd",
            "b3c330b3fe8c435b8e8b641598d02d5e",
            "b16320c22b1f4cb5b8b7bc9599874c61",
            "1c4ff31fd23b48a6bc345d5e58511f50",
            "4354ff53db96438ab6f0e6487601261d",
            "492fb153e64f4403bf6338a442ccdc22",
            "5c99e750110049c5945f2cd03d61bb45",
            "3122f4f3a20c4f4590b5adc5614d2fc9",
            "01c785cd88d644989d50d5dc26c06a33",
            "96b803943668493dba87a6b7e0781b18",
            "6d94783ade084840836933760afeee55",
            "410640673e7249d3b10aad9d16ec6559",
            "33becdfebd214d1cb0dae60cfd27e78a",
            "7e1734bc30764580b4b734d2a6686ae7",
            "8b8a784a9681494090429480c3200b51",
            "824bc3572da84d7f8dbcb5dfd330f0eb",
            "0c6406b46b3d455584da21ac7af5bb86",
            "06e2e781d3264c33a8746eee09eff3d4",
            "1335a0784c97456ca906bd533a5c36b2",
            "593e79da722242c0971de65eff2eefb0",
            "2594a3628fa74add9a6087c475647d52",
            "c2238570bc984010b77d82ff29585c2b",
            "59fee68c85fa45909744b16e5fa53a58"
          ]
        },
        "id": "Oy1x0n2Bz90e",
        "outputId": "ffd7c04b-15d8-4161-b484-55005e13729a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.57.6.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bc82bad87234a09b53e1315e053e70a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "713e64c383ed43629aec89d8ddddd171",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "038e51b470644a249393e569068a5899",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3c330b3fe8c435b8e8b641598d02d5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33becdfebd214d1cb0dae60cfd27e78a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
              "    (layers): ModuleList(\n",
              "      (0): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (1): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (2-31): 30 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(model_name)\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\"\n",
        ")\n",
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaUa6kcEZbBp"
      },
      "source": [
        "## Intent Schema\n",
        "We start with a **controlled taxonomy** to keep intents mutually exclusive and consistent.\n",
        "\n",
        "**Tip:** Decide early on the *granularity* of intents. Too fine-grained leads to sparse data; too coarse leads to ambiguous routing. We've included a pre-set list of intents in ```/content/drive/MyDrive/FICO Analytic Challenge/outbound_intents.csv``` since the evaluation set in week 10 will eventually only include our set of intents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLZRHzzSFGSd"
      },
      "source": [
        "### Scenario Generation\n",
        "\n",
        "The below code block generates scenarios from the input intents CSV and seed scenarios CSV in your ```Fico Analytic Challenge/Data/``` folder. <br> <br>\n",
        "The scenarios are saved to ```Fico Analytic Challenge/Data/``` with filenames like ```scenarios_1.csv```. We will use these scenarios to generate conversations. <br><br>\n",
        "\n",
        "Feel free to play around with the following parameters: <br>\n",
        "<ul>\n",
        "    <li> <b>TEMPERATURE</b> - Controls the \"creativity\" of outputs. Higher values (>0.9) mean more diverse outputs but risk inaccuracies. Lower values (<0.5) mean more predictable, stable outputs but less creativity.\n",
        "    <li> <b>NUM_SCENARIOS</b> - Controls the number of scenarios generated for each intent. Higher NUM_SCENARIOS will generate more data but could lead to formatting issues and lower quality scenarios due to the limited <b>context length</b> of smaller LLMs.\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G74B4JQLXxVl"
      },
      "outputs": [],
      "source": [
        "TEMPERATURE = 0.7\n",
        "NUM_SCENARIOS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc47ai-pZ9Fd"
      },
      "outputs": [],
      "source": [
        "def generate_scenarios(intent, description, seed_df):\n",
        "\n",
        "    seed_df = seed_df[seed_df[\"intent\"] == intent]\n",
        "\n",
        "    examples = f\"EXAMPLE:\\n**Input**: Generate 3 realistic and varied collections outreach scenarios where the resulting customer intent is {intent}.\\n**Output**:\\n\"\n",
        "\n",
        "    random_sample = seed_df.sample(n=3)\n",
        "    for i in range(3):\n",
        "        examples += f\"{i+1}. {random_sample[\"scenario\"].iloc[i]}\\n\"\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a scenario generation engine for outbound banking chatbot conversations.\"\n",
        "        \" The bank initiates contact with the customer about collection on a past-due balance.\"\n",
        "        \" Your task is to generate realistic and logically consistent scenarios in which this outbound contact results in the customer taking a specific action or forming a final intent.\"\n",
        "        \" Each scenario should describe the relevant situation, context, and reasoning that leads from the outbound topic to the customer's concluding intent. The scenarios must reflect believable situations that could occur in real-world banking conversations, including appropriate motivations, financial circumstances, or customer behavior.\\n\"\n",
        "        \"Output strictly as a numbered list with each scenario on a new line. Do not include any commentary or explanation or additional lines.\\n\\n\"\n",
        "        f\"{examples}\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Generate {NUM_SCENARIOS} realistic and varied scenarios where the bank reaches out to the customer about a collection and the resulting customer intent is {intent}. The intent is defined here: {description}.\"}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(DEVICE_TYPE)\n",
        "    output_ids = model.generate(input_ids = inputs, max_new_tokens = 1024, temperature=TEMPERATURE, use_cache = True)\n",
        "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    output = output[output.find(\".assistant\") + len(\".assistant\"):]\n",
        "\n",
        "    scenarios = []\n",
        "    for line in output.split(\"\\n\"):\n",
        "        if line.strip().startswith(tuple(f\"{i}.\" for i in range(1, NUM_SCENARIOS + 1))):\n",
        "            scenario_text = line.split(\".\", 1)[1].strip()\n",
        "            if scenario_text:\n",
        "                scenarios.append(scenario_text)\n",
        "    return scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlEjMtXuaEUI"
      },
      "source": [
        "## Out-of-sample (OOS) examples are user queries that **do not belong to any of the chatbot‚Äôs supported intents**. For a banking chatbot, this might include questions like ‚ÄúWhat‚Äôs the weather today?‚Äù or ‚ÄúBook me a flight to New York‚Äù.\n",
        "\n",
        "Including OOS examples in your dataset is valuable for many reasons:\n",
        "\n",
        "* Improves Robustness\n",
        "\n",
        "  - Without OOS data, the model may try to force-fit every input into one of the known intents, even when it‚Äôs irrelevant. By training with OOS samples labeled as ‚Äúunknown‚Äù or ‚Äúfallback‚Äù, the model learns to gracefully reject unsupported queries.\n",
        "\n",
        "* Enhances User Trust\n",
        "\n",
        "  - It‚Äôs better for the chatbot to say ‚ÄúI‚Äôm not sure I can help with that‚Äù than to give an incorrect or misleading banking response. This prevents confusion and builds user confidence in the system.\n",
        "\n",
        "* Supports Fallback Strategies\n",
        "\n",
        "  - OOS detection enables escalation (e.g., handoff to a human agent) or a redirect (e.g., ‚ÄúI can help with account balances and transfers ‚Äî would you like to try one of those?‚Äù).\n",
        "\n",
        "* Reflects Real-World Behavior\n",
        "\n",
        "  - In production, users will always type unexpected, noisy, or unrelated inputs. Training only on in-domain intents creates a brittle system. OOS examples simulate this reality and make the chatbot resilient.\n",
        "\n",
        "* Helps Reduce Overfitting\n",
        "\n",
        "  - By introducing ‚Äúnegative‚Äù data, the model learns sharper decision boundaries between supported and unsupported queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt4BIPVFZ_AD"
      },
      "outputs": [],
      "source": [
        "def generate_oos_scenarios(all_intents, seed_df):\n",
        "\n",
        "    seed_df = seed_df[seed_df[\"intent\"] == \"FALLBACK\"]\n",
        "\n",
        "    examples = f\"EXAMPLE:\\n**Input**: Generate 3 realistic and varied collections outreach scenarios where the resulting customer intent is FALLBACK.\\n**Output**:\\n\"\n",
        "\n",
        "    random_sample = seed_df.sample(n=3)\n",
        "    for i in range(3):\n",
        "        examples += f\"{i+1}. {random_sample[\"scenario\"].iloc[i]}\\n\"\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a scenario generation engine for outbound banking chatbot conversations.\"\n",
        "        \" The bank initiates contact with the customer about collection on a past-due balance.\"\n",
        "        \" Your task is to generate realistic scenarios in which this outbound contact results in the customer making a request that is outside of the scope of the bank chatbot's capabilities.\"\n",
        "        \" Each scenario should describe the relevant situation and context for the outreach as well as the customer's out-of-scope request. In some scenarios, the customer's request can be loosely related to banking as long as it doesn't touch any of the below intents. In other scenarios, the customer request can be completely unrelated to banking.\\n\"\n",
        "        f\"Defined intents to avoid: {\", \".join(all_intents)}\\n\\n\"\n",
        "        \"Output strictly as a numbered list with each scenario on a new line. Do not include any commentary or explanation or additional lines.\\n\\n\"\n",
        "        f\"{examples}\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Generate {NUM_SCENARIOS} varied scenarios.\"}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
        "    output_ids = model.generate(input_ids = inputs, max_new_tokens = 1024, temperature=TEMPERATURE, use_cache = True)\n",
        "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    output = output[output.find(\".assistant\") + len(\".assistant\"):]\n",
        "\n",
        "    scenarios = []\n",
        "    for line in output.split(\"\\n\"):\n",
        "        if line.strip().startswith(tuple(f\"{i}.\" for i in range(1, NUM_SCENARIOS + 1))):\n",
        "            scenario_text = line.split(\".\", 1)[1].strip()\n",
        "            if scenario_text:\n",
        "                scenarios.append(scenario_text)\n",
        "    return scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUTO62PsaQVP"
      },
      "source": [
        "### This function calls both functions we've defined above: `generate_scenarios` and `generate_oos_scenarios`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiyA9XuKnfq7"
      },
      "outputs": [],
      "source": [
        "def generate_all_scenarios(input_csv=\"collections_intents.csv\", seed_csv=\"seed_scenarios.csv\"):\n",
        "    running_total = 0\n",
        "    input_csv = os.path.join(data_path, input_csv)\n",
        "    seed_data = os.path.join(data_path, seed_csv)\n",
        "    seed_df = pd.read_csv(seed_data)\n",
        "\n",
        "    i = 1\n",
        "    while True:\n",
        "        output_csv = os.path.join(data_path, f\"scenarios_{i}.csv\")\n",
        "        if not os.path.exists(output_csv):\n",
        "            break\n",
        "        i += 1\n",
        "\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    all_intents = list(df[\"intent\"])[:-1]\n",
        "\n",
        "    scenario_data = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        intent = row['intent']\n",
        "        description = row['description']\n",
        "        if intent == \"FALLBACK\":\n",
        "            scenarios = generate_oos_scenarios(all_intents, seed_df)\n",
        "        else:\n",
        "            scenarios = generate_scenarios(intent, description, seed_df)\n",
        "\n",
        "        for sc in scenarios:\n",
        "            new_row = {\n",
        "                \"intent\": intent,\n",
        "                \"scenario\": sc\n",
        "            }\n",
        "            seed_df = pd.concat([seed_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "            scenario_data.append(new_row)\n",
        "\n",
        "        running_total += NUM_SCENARIOS\n",
        "        sys.stdout.write(f\"\\r**GENERATING** Total Scenarios Generated: {running_total}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        pd.DataFrame(scenario_data).to_csv(output_csv, index=False)\n",
        "    print(f\"Saved scenario CSV: {output_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixP8uDXGaTJd"
      },
      "source": [
        "### Now we can call this function to generate all scenarios based on our input intents, which will then be used to generate full conversations!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IRhF6eeXxVl",
        "outputId": "30335955-e3cb-4bdf-c829-6b2c2826379e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**GENERATING** Total Scenarios Generated: 45Saved scenario CSV: /content/drive/MyDrive/FICO Analytic Challenge/Data/scenarios_1.csv\n"
          ]
        }
      ],
      "source": [
        "generate_all_scenarios()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UGKDuIbFGSe"
      },
      "source": [
        "### Conversation Generation\n",
        "\n",
        "The below code block generates conversations from the scenarios CSVs and the seed conversations CSV in ```Fico Analytic Challenge/Data/```. <br> <br>\n",
        "The conversations are saved to ```Fico Analytic Challenge/Data/``` with filenames like ```conversations_1.csv```.<br><br>\n",
        "\n",
        "Feel free to play around with the following parameters: <br>\n",
        "<ul>\n",
        "    <li> <b>TEMPERATURE</b> - As explained earlier, this controls the creativity of outputs. Higher values mean more diversity and creativity at the risk of unpredictable hallucinations. Lower values are safer but less diverse.\n",
        "    <li> <b>SENTIMENTS</b> - This defines the possible sentiments that the user/customer will express. You can change this list to have any number of sentiments you want. Just make sure to update <b>SENTIMENT_PROBS</b> accordingly.\n",
        "    <li> <b>SENTIMENT_PROBS</b> - This is a list of probabilities that defines how likely each sentiment is. For example, if SENTIMENTS is [\"angry\", \"neutral\"] and SENTIMENT_PROBS is [0.3, 0.7], then there is a 30% change the user is angry and a 70% chance the user is neutral. Make sure your probabilities add up to 1.\n",
        "    <li> <b>USER_SPEECH</b> - This defines the possible user speech types. You can change this list to have any number of different speech types (like \"professional\" or \"slang\"). Just make sure to update <b>USER_SPEECH_PROBS</b> accordingly.\n",
        "    <li> <b>USER_SPEECH_PROBS</b> - This is a list of probabilities that defines how likely each user speech type is. For example, if USER_SPEECH is [\"professional\", \"slang\"] and USER_SPEECH_PROBS is [0.4, 0.6], then there is a 40% chance the user speaks professionally and a 60% chance the user uses slang. Make sure your probabilities add up to 1.\n",
        "    <li> <b>NUM_VARIANTS</b> - This controls how many conversations are generated for each scenario. A higher value will generate more data, but you risk having repeptitive conversations if you don't tune the other parameters.\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUb25HoU88V7"
      },
      "outputs": [],
      "source": [
        "# Define sentiments and probabilities\n",
        "TEMPERATURE = 0.7\n",
        "SENTIMENTS = [\"angry\", \"confused\", \"neutral\"]\n",
        "SENTIMENT_PROBS = [0.15, 0.3, 0.55]\n",
        "USER_SPEECH = [\"casual\", \"professional\", \"slang\", \"typos\"]\n",
        "USER_SPEECH_PROBS = [0.5, 0.2, 0.1, 0.2]\n",
        "NUM_VARIANTS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kDXnfVA-WgD"
      },
      "outputs": [],
      "source": [
        "assert len(SENTIMENT_PROBS) == len(SENTIMENTS)\n",
        "assert len(USER_SPEECH_PROBS) == len(USER_SPEECH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf6-FL-FsjUk"
      },
      "outputs": [],
      "source": [
        "all_intents = pd.read_csv(os.path.join(data_path, \"collections_intents.csv\"))\n",
        "all_intents = all_intents[\"intent\"].tolist()\n",
        "\n",
        "def generate_conversation(scenario, intent, sentiment, user_speech):\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a conversation generation engine for outbound banking chatbot interactions. You simulate realistic conversations where the bank initiates contact with the customer about collections on a past-due balance.\\n\"\n",
        "        \"Generate natural, multi-turn dialogues between a helpful, professional banking chatbot and a realistic customer. \"\n",
        "        \"The chatbot should initiate the conversation and stay focused on the context provided in the scenario. \"\n",
        "        f\"The customer messages should express the sentiment '{sentiment}' and speech type '{user_speech}'. However, the customer messages should align with the provided scenario and intent '{intent}' above all else.\\n\"\n",
        "        f\"The conversation must avoid overlapping with any other intents. Specifically, avoid: {\", \".join([i for i in all_intents if i != intent])}\\n\"\n",
        "        \"Avoid including any account numbers or Social Security numbers.\\n\"\n",
        "        \"The chatbot messages should consistently be professional and friendly regardless of the customer's tone.\\n\"\n",
        "        \"Alternate between chatbot and customer messages, prefacing each line with 'Bot:' or 'User:'. Do not include summaries, commentary, or headings ‚Äî output only the dialogue.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"Customer Intent: {intent}\\n\"\n",
        "        f\"Customer Sentiment: {sentiment}\\n\"\n",
        "        f\"Customer Speech Type: {user_speech}\\n\"\n",
        "        f\"Scenario: {scenario}\\n\"\n",
        "        f\"Generate a realistic conversation.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_prompt\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt = False,\n",
        "            tokenize = True,\n",
        "            return_dict = True,\n",
        "            return_tensors = \"pt\",\n",
        "        ).to(\"cuda\"),\n",
        "        max_new_tokens = 1024,\n",
        "        temperature = TEMPERATURE,\n",
        "        top_p = 0.95,\n",
        "        top_k = 128\n",
        "    )\n",
        "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    output = output[output.find(\".assistant\") + len(\".assistant\"):]\n",
        "\n",
        "    return output\n",
        "\n",
        "def generate_oos_conversation(scenario, sentiment, user_speech):\n",
        "\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a conversation generation engine for outbound banking chatbot interactions. You simulate realistic conversations where the bank initiates contact with the customer about collections on a past-due balance and the customer makes an out-of-scope request in response.\\n\"\n",
        "        \"Generate natural, multi-turn dialogues between a helpful, professional banking chatbot and a customer. \"\n",
        "        \"The chatbot should initiate the conversation and stay focused on the context provided in the scenario. \"\n",
        "        f\"The customer messages should express the sentiment '{sentiment}' and speech type '{user_speech}'. However, the customer messages should align with the provided scenario above all else. The purpose of the scenario is to describe a customer that is making an out-of-scope request that the banking chatbot can't fulfill.\\n\"\n",
        "        \"The chatbot messages should consistently be professional and friendly regardless of the customer's tone.\\n\"\n",
        "        \"Alternate between chatbot and customer messages, prefacing each line with 'Bot:' or 'User:'. Do not include summaries, commentary, or headings ‚Äî output only the dialogue.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"Customer Intent: OUT_OF_SCOPE\"\n",
        "        f\"Customer Sentiment: {sentiment}\\n\"\n",
        "        f\"Customer Speech Type: {user_speech}\\n\"\n",
        "        f\"Scenario: {scenario}\\n\"\n",
        "        f\"Generate a realistic conversation.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_prompt\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt = False,\n",
        "            tokenize = True,\n",
        "            return_dict = True,\n",
        "            return_tensors = \"pt\",\n",
        "        ).to(\"cuda\"),\n",
        "        max_new_tokens = 1024,\n",
        "        temperature = TEMPERATURE,\n",
        "        top_p = 0.95,\n",
        "        top_k = 128\n",
        "    )\n",
        "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    output = output[output.find(\".assistant\") + len(\".assistant\"):]\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def generate_all_conversations():\n",
        "\n",
        "    running_total = 0\n",
        "    pattern = re.compile(r\"^scenarios_(\\d+)\\.csv$\")\n",
        "\n",
        "    csv_files = []\n",
        "    for f in os.listdir(data_path):\n",
        "        if pattern.match(f):\n",
        "            csv_files.append(os.path.join(data_path, f))\n",
        "\n",
        "    if csv_files:\n",
        "        df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No matching scenarios files found in the specified data_path.\")\n",
        "\n",
        "    i = 1\n",
        "    while True:\n",
        "        output_csv = os.path.join(data_path, f\"conversations_{i}.csv\")\n",
        "        if not os.path.exists(output_csv):\n",
        "            break\n",
        "        i += 1\n",
        "\n",
        "    all_convos = []\n",
        "\n",
        "    for idx, row in df.sample(frac=1).iterrows():\n",
        "        intent = row['intent']\n",
        "        scenario = row['scenario']\n",
        "\n",
        "        for variant_id in range(1, NUM_VARIANTS + 1):\n",
        "\n",
        "            sentiment = random.choices(SENTIMENTS, weights=SENTIMENT_PROBS, k=1)[0]\n",
        "            user_speech = random.choices(USER_SPEECH, weights=USER_SPEECH_PROBS, k=1)[0]\n",
        "            if intent == \"FALLBACK\":\n",
        "                conv_text = generate_oos_conversation(scenario, sentiment, user_speech)\n",
        "            else:\n",
        "                conv_text = generate_conversation(scenario, intent, sentiment, user_speech)\n",
        "\n",
        "            num_turns = len(re.findall(r\"(User:|Bot:)\", conv_text))\n",
        "            if num_turns <= 1:\n",
        "                continue\n",
        "\n",
        "            all_convos.append({\n",
        "                \"intent\": intent,\n",
        "                \"scenario\": scenario,\n",
        "                \"conversation_text\": conv_text,\n",
        "                \"sentiment\": sentiment,\n",
        "                \"user_speech_type\": user_speech\n",
        "            })\n",
        "            running_total += 1\n",
        "\n",
        "            sys.stdout.write(f\"\\r**GENERATING** Total Conversations Generated: {running_total}\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        pd.DataFrame(all_convos).to_csv(output_csv, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5kzr4pnaeEl"
      },
      "source": [
        "### Now we're ready to start generating our synthetic data based on our previously defined scenarios!\n",
        "\n",
        "Pay attention to the `while True` loop above: every time you generate a new set of conversations, it will generate a new .csv file with a new index. Make sure to keep track of your experimental parameters as you're creating new sets of data when trying to optimize for synthetic data quality and intent fidelity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fog7dwGXxVm",
        "outputId": "de658ee7-1f22-4390-d2a3-666f57633a3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**GENERATING** Total Conversations Generated: 225"
          ]
        }
      ],
      "source": [
        "generate_all_conversations()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}